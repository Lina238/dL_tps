{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7meu-pps3QMc"
      },
      "source": [
        "# Utilisation des CNNs avec Keras sur des données d'expression faciale"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7-diCsctWfSh"
      },
      "source": [
        "## Vérification de l'utilisation de GPU\n",
        "\n",
        "Allez dans le menu `Exécution > Modifier le type d'execution` et vérifiez que l'on est bien en Python 3 et que l'accélérateur matériel est configuré sur « GPU »."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-S9vf7TtjjMa"
      },
      "outputs": [],
      "source": [
        "!nvidia-smi"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ifgZIuXxPc57"
      },
      "source": [
        "## Téléchargement du dataset d'expressions faciales depuis un repo git"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-nh2my7ePd6G"
      },
      "outputs": [],
      "source": [
        "!git clone https://github.com/muxspace/facial_expressions.git\n",
        "!ls\n",
        "!head facial_expressions/data/legend.csv"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w_vshNHAPj6y"
      },
      "source": [
        "## Import de TensorFlow et des autres librairies nécessaires"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xDqlPGuPVuZn"
      },
      "outputs": [],
      "source": [
        "import itertools\n",
        "import pathlib\n",
        "import typing\n",
        "\n",
        "import cv2\n",
        "import IPython\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy\n",
        "import pandas\n",
        "import seaborn\n",
        "import sklearn.metrics\n",
        "import sklearn.model_selection\n",
        "import sklearn.utils\n",
        "import tensorflow.keras as keras"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "glVauu2SkG1A"
      },
      "source": [
        "## Chargement des données\n",
        "\n",
        "*Après une rapide étude du dataset à l'aide de commandes usuelles du terminal, implémentez les fonctions de chargement du dataset. Vous pourrez utiliser [`pandas.read_csv`](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.read_csv.html) pour charger un CSV dans une [`pandas.DataFrame`](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.html).*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3eDbOBSHkuSw"
      },
      "outputs": [],
      "source": [
        "# Votre code ici\n",
        "def load_data(csv_path: pathlib.Path, shuffle: bool = True\n",
        "             ) -> typing.Tuple[numpy.ndarray, numpy.ndarray]:\n",
        "  return numpy.array([]), numpy.array([])\n",
        "\n",
        "\n",
        "images, labels = load_data(\n",
        "    pathlib.Path(\"facial_expressions\") / \"data\" / \"legend.csv\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uj7vdViMlF3D"
      },
      "source": [
        "### Solution"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Wf2HIPYfk0WX"
      },
      "outputs": [],
      "source": [
        "images, labels = load_data(\n",
        "    pathlib.Path(\"facial_expressions\") / \"data\" / \"legend.csv\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1HSxBIIXlMZJ"
      },
      "source": [
        "## Exploration du dataset\n",
        "\n",
        "*Affichez les caractéristiques du dataset suivantes :*\n",
        "\n",
        "- *La taille des données*\n",
        "- *Le nombre d'exemple dans chaque classe*\n",
        "- *Quelques exemples annotés*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rV6v_F-lkj0r"
      },
      "outputs": [],
      "source": [
        "# Votre code ici"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CplV6KtynBbQ"
      },
      "source": [
        "### Solution"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YBbfuELPnnVn"
      },
      "source": [
        "## Séparation train/test\n",
        "\n",
        "Pour pouvoir évaluer notre modèle, il faut mettre de côté une partie des données. On ne les utilisera pas pendant l'apprentissage."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "o9JQ6LiwNC47"
      },
      "outputs": [],
      "source": [
        "# Séparation en train et test (la séparation en train/valid est faite\n",
        "# automatiquement dans model.fit())\n",
        "# On fera une séparation 80/20 en respectant les proportions des classes\n",
        "train_images, test_images, train_labels, test_labels = (\n",
        "    sklearn.model_selection.train_test_split(images,\n",
        "                                             labels,\n",
        "                                             test_size=0.2,\n",
        "                                             stratify=labels))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zbNJmMGDn3PF"
      },
      "source": [
        "## Apprentisage\n",
        "\n",
        "- *Définissez un modèle*\n",
        "- *Entraînez votre modèle avec `model.fit(…)`*\n",
        "- *Affichez vos courbes d'apprentissage*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0kLOvELXoP6I"
      },
      "outputs": [],
      "source": [
        "# Votre code ici\n",
        "model = keras.models.Sequential()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5Z0x2FNEory5"
      },
      "source": [
        "### Solution"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gvXQHYY9oyrb"
      },
      "source": [
        "## Évaluation des performances en test\n",
        "\n",
        "*Utilisez `model.evaluate(…)` pour mesurer les performances de votre modèle sur l'ensemble de test.*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c8MQxeGwo4h9"
      },
      "outputs": [],
      "source": [
        "# Votre code ici"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gq8vcEL8o78Z"
      },
      "source": [
        "### Solution"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AxFZ0wrCpApj"
      },
      "source": [
        "## Calcul et affichage de la matrice de confusion\n",
        "\n",
        "*Utilisez [`sklearn.metrics.confusion_matrix`](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.confusion_matrix.html) pour calculer la matrice de confusion de votre modèle sur les données de test, puis utilisez [`seaborn.heatmap`](https://seaborn.pydata.org/generated/seaborn.heatmap.html) pour l'afficher.*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "N5JXgtkdpjZj"
      },
      "outputs": [],
      "source": [
        "# Votre code ici"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N4jMT76Wpla-"
      },
      "source": [
        "### Solution"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uyXmbpFDsLh_"
      },
      "source": [
        "## Avec des pondération pour les classes\n",
        "\n",
        "Il est possible de donner un argument `class_weight` à `model.fit`. Cet argument correspond à un facteur multiplicatif dans la loss à appliquer au score de chaque classe.\n",
        "\n",
        "*Utilisez [`sklearn.utils.class_weight.compute_class_weight`](https://scikit-learn.org/stable/modules/generated/sklearn.utils.class_weight.compute_class_weight.html) pour calculer les poids à attribuer à chaque classe.*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6sHnihoAqfR4"
      },
      "outputs": [],
      "source": [
        "class_weights = sklearn.utils.class_weight.compute_class_weight(\n",
        "    \"balanced\", numpy.unique(train_labels), train_labels)\n",
        "print(class_weights)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s4HUpU_C2OSa"
      },
      "source": [
        "## Clipping des pondérations\n",
        "\n",
        "Dans un premier temps, il peut être intéressant de vérifier que les poids de classe améliorent les performances du modèle pour les classes sous-représentées sans utiliser toutefois des pondérations trop fortes.\n",
        "\n",
        "*Utilisez [`numpy.clip`](https://numpy.org/doc/stable/reference/generated/numpy.clip.html) pour restreindre les valeurs des pondérations à $[0, 5]$.*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "S6tfzJsF2y2k"
      },
      "outputs": [],
      "source": [
        "# Votre code ici"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "esUX5Wmo2zr0"
      },
      "source": [
        "### Solution"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0VGaWFeI2jbA"
      },
      "source": [
        "## Utilisation des pondérations\n",
        "\n",
        "*Re-contruisez votre modèle et entraînez le avec les pondérations calculées, puis évaluez-le.*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wAA7sUyP2qlO"
      },
      "outputs": [],
      "source": [
        "# Votre code ici"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uNVh7i2s2rj_"
      },
      "source": [
        "### Solution"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4JbdlLNyuNRs"
      },
      "outputs": [],
      "source": [
        "evaluate(model, test_images, test_labels)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# TP - Classification d'Expressions Faciales avec CNN\n",
        "\n",
        "# Étape 1 : Vérification GPU et Imports\n",
        "import os\n",
        "import pathlib\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import cv2\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import sklearn.model_selection\n",
        "import sklearn.metrics\n",
        "import sklearn.utils\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "\n",
        "# Vérification GPU\n",
        "print(\"GPU disponible :\", tf.test.is_gpu_available())\n",
        "\n",
        "# Étape 2 : Chargement des données\n",
        "def load_data(csv_path, images_dir):\n",
        "    \"\"\"\n",
        "    Charger les images et les labels à partir d'un fichier CSV\n",
        "\n",
        "    Args:\n",
        "    - csv_path: Chemin vers le fichier CSV des labels\n",
        "    - images_dir: Répertoire contenant les images\n",
        "\n",
        "    Returns:\n",
        "    - images: Tableau numpy des images\n",
        "    - labels: Tableau numpy des labels\n",
        "    \"\"\"\n",
        "    # Lecture du CSV\n",
        "    df = pd.read_csv(csv_path)\n",
        "\n",
        "    # Préparation des listes pour stocker images et labels\n",
        "    images = []\n",
        "    labels = []\n",
        "\n",
        "    # Parcours du dataframe\n",
        "    for index, row in df.iterrows():\n",
        "        # Chemin complet de l'image\n",
        "        img_path = os.path.join(images_dir, row['filename'])\n",
        "\n",
        "        # Charger l'image en niveaux de gris et redimensionner\n",
        "        img = cv2.imread(img_path, cv2.IMREAD_GRAYSCALE)\n",
        "        if img is not None:\n",
        "            img = cv2.resize(img, (48, 48))  # Taille standard pour les CNN\n",
        "            images.append(img)\n",
        "            labels.append(row['emotion'])\n",
        "\n",
        "    return np.array(images), np.array(labels)\n",
        "\n",
        "# Charger les données\n",
        "images_dir = 'facial_expressions/data/images'\n",
        "csv_path = 'facial_expressions/data/legend.csv'\n",
        "images, labels = load_data(csv_path, images_dir)\n",
        "\n",
        "# Étape 3 : Exploration du dataset\n",
        "# Distribution des classes\n",
        "unique_labels, label_counts = np.unique(labels, return_counts=True)\n",
        "plt.figure(figsize=(10, 5))\n",
        "plt.bar(unique_labels, label_counts)\n",
        "plt.title('Distribution des Classes')\n",
        "plt.xlabel('Émotions')\n",
        "plt.ylabel('Nombre d\\'images')\n",
        "plt.show()\n",
        "\n",
        "# Affichage de quelques images\n",
        "plt.figure(figsize=(15, 5))\n",
        "for i in range(10):\n",
        "    plt.subplot(2, 5, i+1)\n",
        "    plt.imshow(images[i], cmap='gray')\n",
        "    plt.title(f'Émotion: {labels[i]}')\n",
        "    plt.axis('off')\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Prétraitement des données\n",
        "# Normalisation et reshape\n",
        "images = images.reshape(-1, 48, 48, 1) / 255.0\n",
        "\n",
        "# Encodage one-hot des labels\n",
        "labels = keras.utils.to_categorical(labels)\n",
        "\n",
        "# Séparation train/test\n",
        "train_images, test_images, train_labels, test_labels = (\n",
        "    sklearn.model_selection.train_test_split(\n",
        "        images, labels, test_size=0.2, stratify=np.argmax(labels, axis=1)\n",
        "    )\n",
        ")\n",
        "\n",
        "# Construction du modèle CNN\n",
        "def create_cnn_model(input_shape, num_classes):\n",
        "    model = keras.Sequential([\n",
        "        # Première couche de convolution\n",
        "        keras.layers.Conv2D(32, (3, 3), activation='relu', input_shape=input_shape),\n",
        "        keras.layers.BatchNormalization(),\n",
        "        keras.layers.MaxPooling2D((2, 2)),\n",
        "        keras.layers.Dropout(0.25),\n",
        "\n",
        "        # Deuxième couche de convolution\n",
        "        keras.layers.Conv2D(64, (3, 3), activation='relu'),\n",
        "        keras.layers.BatchNormalization(),\n",
        "        keras.layers.MaxPooling2D((2, 2)),\n",
        "        keras.layers.Dropout(0.25),\n",
        "\n",
        "        # Troisième couche de convolution\n",
        "        keras.layers.Conv2D(128, (3, 3), activation='relu'),\n",
        "        keras.layers.BatchNormalization(),\n",
        "        keras.layers.MaxPooling2D((2, 2)),\n",
        "        keras.layers.Dropout(0.25),\n",
        "\n",
        "        # Couche de flattening\n",
        "        keras.layers.Flatten(),\n",
        "\n",
        "        # Couches fully connected\n",
        "        keras.layers.Dense(512, activation='relu'),\n",
        "        keras.layers.BatchNormalization(),\n",
        "        keras.layers.Dropout(0.5),\n",
        "\n",
        "        # Couche de sortie\n",
        "        keras.layers.Dense(num_classes, activation='softmax')\n",
        "    ])\n",
        "\n",
        "    return model\n",
        "\n",
        "# Création du modèle\n",
        "model = create_cnn_model((48, 48, 1), num_classes=labels.shape[1])\n",
        "\n",
        "# Compilation du modèle\n",
        "model.compile(\n",
        "    optimizer='adam',\n",
        "    loss='categorical_crossentropy',\n",
        "    metrics=['accuracy']\n",
        ")\n",
        "\n",
        "# Calcul des poids de classe\n",
        "class_weights = sklearn.utils.class_weight.compute_class_weight(\n",
        "    'balanced',\n",
        "    classes=np.unique(np.argmax(labels, axis=1)),\n",
        "    y=np.argmax(labels, axis=1)\n",
        ")\n",
        "\n",
        "# Clip des poids de classe\n",
        "class_weights = np.clip(class_weights, 0, 5)\n",
        "class_weights_dict = dict(enumerate(class_weights))\n",
        "\n",
        "# Entraînement du modèle\n",
        "history = model.fit(\n",
        "    train_images, train_labels,\n",
        "    epochs=50,\n",
        "    batch_size=64,\n",
        "    validation_split=0.2,\n",
        "    class_weight=class_weights_dict,\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "# Évaluation du modèle\n",
        "test_loss, test_accuracy = model.evaluate(test_images, test_labels)\n",
        "print(f'Test accuracy: {test_accuracy * 100:.2f}%')\n",
        "\n",
        "# Prédictions\n",
        "y_pred = model.predict(test_images)\n",
        "y_pred_classes = np.argmax(y_pred, axis=1)\n",
        "y_true_classes = np.argmax(test_labels, axis=1)\n",
        "\n",
        "# Matrice de confusion\n",
        "cm = sklearn.metrics.confusion_matrix(y_true_classes, y_pred_classes)\n",
        "plt.figure(figsize=(10, 8))\n",
        "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')\n",
        "plt.title('Matrice de Confusion')\n",
        "plt.xlabel('Prédictions')\n",
        "plt.ylabel('Vraies Classes')\n",
        "plt.show()\n",
        "\n",
        "# Courbes d'apprentissage\n",
        "plt.figure(figsize=(12, 4))\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.plot(history.history['accuracy'], label='Train Accuracy')\n",
        "plt.plot(history.history['val_accuracy'], label='Validation Accuracy')\n",
        "plt.title('Accuracy')\n",
        "plt.legend()\n",
        "\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.plot(history.history['loss'], label='Train Loss')\n",
        "plt.plot(history.history['val_loss'], label='Validation Loss')\n",
        "plt.title('Loss')\n",
        "plt.legend()\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "V4h7nm1AUf5T"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "private_outputs": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.13"
    },
    "gpuClass": "standard"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}